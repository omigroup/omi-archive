{"data":{"repository":{"discussions":{"totalCount":130,"nodes":[{"id":"D_kwDOFwL-vM4Aid0Z","category":{"name":"Ideas"},"upvoteCount":2,"updatedAt":"2025-10-16T18:56:00Z","createdAt":"2025-10-16T18:09:22Z","number":252,"title":"Exploring Audio Reactivity in glTF","body":"I wanted to start a conversation around a concept that's becoming increasingly prominent in social virtual worlds like VRChat and Resonite: **audio reactivity**. We're seeing these incredible, immersive spaces where the world itself‚Äîthe floors, the walls, the very air‚Äîpulses and vibes in sync with live music. üéµ\r\n\r\nRight now, creating this content is a highly platform-specific and technical endeavor, often requiring deep knowledge of proprietary shader systems like AudioLink. This locks incredible creative potential inside particular gardens.\r\n\r\nSo, the food-for-thought question is: **What if we could embed the *intent* for audio reactivity directly into a glTF asset?**\r\n\r\nThe goal wouldn't be to standardize the *implementation*, but to create a common language for an asset to say, \"When you hear a beat, I should flash,\" or \"When the bass hits, I should glow.\" This would allow for:\r\n\r\n  * **Graceful Degradation:** The asset would look like a normal, beautiful static model in any standard glTF viewer.\r\n  * **Progressive Enhancement:** When loaded into a capable engine, the asset would \"wake up\" and come alive with the music.\r\n\r\n-----\r\n\r\n### A \"Declarative\" Approach: Describing What, Not How\r\n\r\nSince we can't ship shaders or scripts in glTF, the approach would be purely declarative. An extension would simply provide metadata that **binds standardized audio signals** to an object's existing properties. A runtime could then interpret this metadata and hook it up to its native audio analysis system.\r\n\r\nHere‚Äôs a possible tiered approach, starting simple and building up in complexity.\r\n\r\n-----\r\n\r\n### Tier 1: Living Materials (`OMI_audio_reactive_material`) üí°\r\n\r\nThis is the foundation. It would allow artists to make a material's properties react to sound. Imagine an artist in Blender setting up a crystal material. They wouldn't write code; they'd just choose from a dropdown.\r\n\r\n**The Vision:** A crystal that throbs with a soft, emissive glow in time with the music's bassline.\r\n\r\nUnder the hood, the glTF `material` definition could look something like this:\r\n\r\n```json\r\n\"extensions\": {\r\n  \"OMI_audio_reactive_material\": {\r\n    \"bindings\": [\r\n      {\r\n        \"signal\": \"BASS\",\r\n        \"target\": \"emissiveFactor\",\r\n        \"mode\": \"MULTIPLY\",\r\n        \"factor\": 5.0\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n  * **`signal`**: A standard enum like `BASS`, `TREBLE`, `BEAT`, or `LOUDNESS`. This is the universal translator.\r\n  * **`target`**: A standard glTF property like `emissiveFactor`.\r\n  * **`mode`**: A simple instruction like `ADD` or `MULTIPLY`.\r\n\r\nA VRChat/Resonite importer could see this and automatically wire it up to an AudioLink shader. Another engine could use its own audio analyzer. The artist's creative intent is preserved across platforms.\r\n\r\n-----\r\n\r\n### Tier 2: Moving the World (`OMI_audio_reactive_node`) üõ†Ô∏è\r\n\r\nThe next step would be applying the same logic to a node's transform, allowing objects to move, scale, or rotate with the music.\r\n\r\n**The Vision:** A series of pillars on a stage that gently \"breathe\" in and out, scaling up slightly on every beat.\r\n\r\nThe `node` definition could be extended like so:\r\n\r\n```json\r\n\"extensions\": {\r\n  \"OMI_audio_reactive_node\": {\r\n    \"bindings\": [\r\n      {\r\n        \"signal\": \"BEAT\",\r\n        \"target\": \"scale\",\r\n        \"mode\": \"ADD\",\r\n        \"factor\": [0.0, 0.1, 0.0] \r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nIn a non-reactive viewer, you just see pillars. In a reactive world, the stage feels alive and architectural elements become part of the performance.\r\n\r\n-----\r\n\r\n### Tier 3: Complex Choreography (`OMI_audio_reactive_animation`) ‚ú®\r\n\r\nFor the ultimate level of creative control, we could allow an audio signal to drive the playback of a standard glTF animation. This lets artists author complex, nuanced reactions using the animation tools they already know.\r\n\r\n**The Vision:** A complex mechanical flower whose petals open and close based on the overall loudness of a song, revealing a glowing core during the chorus.\r\n\r\nThe artist would create a normal 0-to-10-second animation of the flower opening. The extension would then \"hijack\" the animation's timeline.\r\n\r\n```json\r\n\"animations\": [\r\n  {\r\n    \"name\": \"Flower_Bloom_Animation\",\r\n    ...\r\n    \"extensions\": {\r\n      \"OMI_audio_reactive_animation\": {\r\n        \"signal\": \"LOUDNESS\"\r\n      }\r\n    }\r\n  }\r\n]\r\n```\r\n\r\nNow, when the music is quiet (`LOUDNESS` = 0.0), the animation is at its 0-second mark (flower closed). When the music swells to its peak (`LOUDNESS` = 1.0), the animation scrubs to its end (flower fully open).\r\n\r\n-----\r\n\r\n### The Big Picture\r\n\r\nThis kind of extension could empower creators to build richer, more dynamic, and more valuable assets that work across the open metaverse. It respects the glTF philosophy of being a portable, declarative format while opening the door to the next generation of interactive content.\r\n\r\nWould love to hear people's thoughts on this. Is this a viable direction? What are the potential pitfalls? Are there other signals or modes that would be essential for creators?\r\n\r\nLet's discuss\\!","author":{"login":"humbletim"},"comments":{"nodes":[{"id":"DC_kwDOFwL-vM4A4Fcv","author":{"login":"aaronfranke"},"body":"`\"target\"` should probably be a glTF JSON pointer to allow it to reference any property, such as `/nodes/0/scale`."}]},"labels":{"nodes":[]}},{"id":"D_kwDOFwL-vM4AgbK0","category":{"name":"Ideas"},"upvoteCount":2,"updatedAt":"2025-07-10T17:27:31Z","createdAt":"2025-06-26T20:03:33Z","number":250,"title":"IDEA RFC: OMI glTF Extension Sprint: A 4-Week Pilot Program","body":"Per casual discussions at today's glTF subgroup meeting, here is an initial way to consider framing the four-week structured journey concept. An idea here is to imagine that journey from idea to artifact in just four one-hour sessions. Each session is meant to deliver a tangible outcome, so even if people only attend one or two, they leave with something concrete.\r\n\r\nThe primary goal is to create a low-friction, high-value way to \"practice\" the glTF extension lifecycle, demystifying the process and hopefully providing a launchpad for more sustained efforts.\r\n\r\nBelow is a rough, first-cut of what that could look like.\r\n\r\n***\r\n\r\n### Proposed 4-Week Sprint Outline\r\n\r\nThe chosen \"toy\" extension for this sprint is `OMI_node_metadata`, designed for maximum simplicity to keep the focus on the *process*, not complex technical details.\r\n\r\n#### **Week 1: The Spark - From Zero to a Concrete Idea**\r\n* **Topic:** What is a glTF extension, really?\r\n* **Activity:** A 5-minute deconstruction of an existing simple extension to identify the key parts (spec, schema, example). Then, collaboratively write a one-paragraph proposal for our `OMI_node_metadata` concept.\r\n* **Outcome:** A shared document with a clear, concise proposal.\r\n\r\n#### **Week 2: The Blueprint - Creating the Schema**\r\n* **Topic:** Defining the \"rules\" for our extension in a machine-readable way.\r\n* **Activity:** A brief intro to JSON Schema, followed by a live, group-editing session to create the `OMI_node_metadata.schema.json` file. We'll use an online validator to check our work.\r\n* **Outcome:** A valid schema file that formally defines our extension's data.\r\n\r\n#### **Week 3: The Proof - Hacking a Live glTF**\r\n* **Topic:** Making it real by creating a tangible asset. This is the \"Aha!\" moment.\r\n* **Activity:** As a group, manually edit a simple `.gltf` file in a text editor to add the necessary `extensionsUsed` fields and the `OMI_node_metadata` data block to a node.\r\n* **Outcome:** A valid glTF model that contains our custom extension data.\r\n\r\n#### **Week 4: The Story & The Next Step**\r\n* **Topic:** Documenting what we did and gauging interest for the future.\r\n* **Activity:** Collaboratively write a \"micro-spec\" (`README.md`) that summarizes the extension and shows an example. The second half of the session would be a retrospective and a call to action to see who might be interested in a longer, more in-depth program.\r\n* **Outcome:** A complete \"pilot program package\" (proposal, schema, example, summary) and a clear path to gauge interest for a larger initiative.\r\n\r\n***\r\n\r\nThis is just a starting point for discussion. Would love to hear initial thoughts from the group:\r\n\r\n* Does this 4-week structure feel like a practical and valuable \"sampler\"?\r\n* Would this be an engaging way to learn the ropes of extension development?\r\n* Any suggestions on the activities or the proposed \"toy\" extension?\r\n","author":{"login":"humbletim"},"comments":{"nodes":[{"id":"DC_kwDOFwL-vM4Az4k-","author":{"login":"indiebio"},"body":"dropping stuff here for media \r\nusing the prompt: Please create an image combining the metaverse with glTF and July, in a layered paper look, using only the colours \r\n#b337e2, \r\n#eb44d1, \r\n#d13ed9, \r\n#932fec, \r\n#004aad\r\n\r\nProbably need to add OMIgroup and stuff. Probably better to use canva.\r\n<img src=\"https://github.com/user-attachments/assets/372a3315-ff71-4355-8e76-525d0cf79dfc\" height=200px align=left>\r\n\r\nadding Please create an image about a snowglobe containing low poly 3D ponies, in a layered paper look, using only the colours \r\n#b337e2, #eb44d1, #d13ed9, #932fec, #004aad (snowglobe because of the https://omigroup.github.io/three-omi/ snowglobe which I love, and also ponies which I love)\r\n<img src=\"https://github.com/user-attachments/assets/a05baa10-367f-4353-8a9e-84cbfe27f2c3\" height=200px align=left>\r\n\r\n\r\n"},{"id":"DC_kwDOFwL-vM4A0WQK","author":{"login":"indiebio"},"body":"Resources to use for the content of the course:\r\nhttps://github.com/toddeTV/talk-2025-06-04-frontend-nation\r\nhttps://deepwiki.com/omigroup/gltf-extensions\r\nhttps://omigroup.org/notes-from-gltf-interactivity-extension/\r\n\r\n"},{"id":"DC_kwDOFwL-vM4A0WQw","author":{"login":"indiebio"},"body":"Week 1: The Spark - From Zero to a Concrete Idea\r\nTopic: What is a glTF extension, really?\r\n - what is glTF\r\n - what is an extension\r\n - why are we working on glTF extensions?\r\nActivity: A 5-minute deconstruction of an existing simple extension to identify the key parts (spec, schema, example). Then, collaboratively write a one-paragraph proposal for our OMI_node_metadata concept.\r\nOutcome: A shared document with a clear, concise proposal."}]},"labels":{"nodes":[]}},{"id":"D_kwDOFwL-vM4AeyNS","category":{"name":"General"},"upvoteCount":1,"updatedAt":"2025-03-12T06:55:36Z","createdAt":"2025-03-12T06:55:35Z","number":247,"title":"OMI glTF Working Group Meeting 2025-03-13","body":"This meeting is on 2025-03-13 at 16:00 UTC / 9:00 AM PDT in the [OMI Discord](https://discord.gg/J4vyAWFgkj) within the \"Weekly Meeting\" voice channel. During the meeting, we will be using the #omi-gltf-extensions text channel to post links and engage in further discussion.\r\n\r\nTo be notified of this meeting and others, subscribe to the [OMI Meetings and Events Calendar](https://calendar.google.com/calendar/u/1?cid=Y18wZHB1Z2Y5ZjgzZXE0cWVrbWI2b21xYmptZ0Bncm91cC5jYWxlbmRhci5nb29nbGUuY29t) or add yourself to the @omi-gltf-subgroup role in the #roles channel of the OMI Discord.\r\n\r\n\r\n\r\n\r\n\r\n","author":{"login":"aaronfranke"},"comments":{"nodes":[]},"labels":{"nodes":[]}},{"id":"D_kwDOFwL-vM4AesyW","category":{"name":"General"},"upvoteCount":1,"updatedAt":"2025-03-06T17:36:25Z","createdAt":"2025-03-06T17:20:05Z","number":246,"title":"OMI glTF Working Group Meeting 2025-03-06","body":"This meeting is on 2025-03-06 at 17:00 UTC / 9:00 AM PST in the [OMI Discord](https://discord.gg/J4vyAWFgkj) within the \"Weekly Meeting\" voice channel. During the meeting, we will be using the #omi-gltf-extensions text channel to post links and engage in further discussion.\r\n\r\nTo be notified of this meeting and others, subscribe to the [OMI Meetings and Events Calendar](https://calendar.google.com/calendar/u/1?cid=Y18wZHB1Z2Y5ZjgzZXE0cWVrbWI2b21xYmptZ0Bncm91cC5jYWxlbmRhci5nb29nbGUuY29t) or add yourself to the @omi-gltf-subgroup role in the #roles channel of the OMI Discord.\r\n\r\n\r\n\r\n\r\n\r\n","author":{"login":"aaronfranke"},"comments":{"nodes":[{"id":"DC_kwDOFwL-vM4AvXg8","author":{"login":"aaronfranke"},"body":"Last week and in weeks before that we discussed OMI_environment_sky https://github.com/omigroup/gltf-extensions/discussions/245\r\n\r\nThis week we are voting to merge OMI_environment_sky https://github.com/omigroup/gltf-extensions/pull/244"}]},"labels":{"nodes":[]}}]}}}}
